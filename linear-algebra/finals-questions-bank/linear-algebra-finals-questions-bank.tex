\documentclass{article}
\usepackage{amsmath, amsthm, amsfonts}

\newtheorem{innercustomthm}{Theorem}
\newenvironment{customthm}[1]
  {\renewcommand\theinnercustomthm{#1}\innercustomthm}
  {\endinnercustomthm}

\author{Mostafa Hassanein}
\title{Linear Algebra (MTH401) Finals Questions Bank}
\date{14 January 2024}
\begin{document}

\maketitle

\newpage

\section*{Proofs}

\begin{customthm}{1}[2023.S(1.A.i)]
  $ $

  i. Disprove: $W$ is a subspace of the vector space $V$ and $v \in V$. Then, the set defined by $v+W = \{ v+w: w \in W \}$, is a subspace of $V$.
  \newline

  ii. Under what condition is $v+W$ a subspace of $V$?
\end{customthm}

\begin{proof}
  $ $

  i. By counterexample: 
  
  Let $V = R^2$, $W = \{ w \in V: w = (x,0) \}$, and $v = (0, 1)$.
  
  \noindent
  $\implies v+W = \{ u \in V: u = (x, 1) \}$.

  \noindent
  $\implies 0_v = (0, 0) \notin v+W$

  \noindent
  $\implies$ v+W is not a subspace.
  \newline

  ii. It's clear that if $v \in W$, then $v+W = W$, which is a subspace of $V$ as desired.

  If, however, $v \notin W$, then, by the counterexample provided, $v+W$ is not a subspace of $V$.

  Therefore, the sufficient and necessary condition is: $v \in W$.

\end{proof}

\noindent
Note: The transformation $v+W = \{ v+w: w \in W \}$ is an \underline{\textbf{affine transformation}}, which is not—necessarily—linear.

\newpage

\begin{customthm}{2}[2023.S(1.A.ii)]
  $ $

  Let $x_1,\ldots,x_{n+1}$ be distinct elements of $F$. Then, the functions $f_i(x)= \Pi_{k=1,k \neq i}^{n+1} \frac{(x-x_k)}{(x_i-x_k)}$ for $i=1,\ldots,n+1$ form a basis for $P_n(F)$.
  \newline

  Note to self: A possible source of confusion here is that $F$ is not an infinite field, rather it's a finite field given by $F = \{x_1,\ldots,x_n \}$.
\end{customthm}

\begin{proof}
  $ $

  Since $dim(P_n) = n+1$ and we have $n+1$ functions/vectors, then it's sufficient to check either one of the following conditions: 
  
  \qquad i. The $n+1$ functions/vectors span $P_n$

  \qquad ii. The $n+1$ functions/vectors are linearly independent.
  \newline

  We check (ii). Let $x_j, a_i \in \{x_1,\ldots,x_{n+1}\}$, then:
  \begin{align*}
    0_v &= \sum_{i=1}^{n+1} a_i f_i(x_j) &&\\
    &= \sum_{i=1}^{n+1} a_i \Pi_{k=1, k \neq i}^{n+1}\frac{x_j - x_k}{x_i - x_k} &&\\
    &= \sum_{i=1}^{n+1} a_i \delta_{ij} &&\\
    &= a_j.
  \end{align*}

  \noindent
  $\implies \forall j \in \{1,\ldots,n+1\}: \  a_j = 0$.

  \noindent
  $\implies (f_1,\ldots,f_n)$ are linearly independent.

  \noindent
  $\implies (f_1,\ldots,f_n)$ is a basis for $P_n$.

\end{proof}

\noindent
Notes:

\quad - The tricky part in this question is to realize that the complicated form of $f_i$ reduces to $\delta_{ij}$ when applying $f_i$ to $x_j$.

\quad - Intuitively, this is because the output of applying $f_i(x)$ to some $x_j \in F$ must be one of the finite scalars in $F$.

\quad - I suppose this result can be generalized to all function spaces (not just polynomials) over a finite field. The generalization can be stated as follows: Any function space over a finite field of cardinality $n$ has a basis given by the functions $f_i$ where $f_i(x_j) := \delta_{ij}$ for $i,j \in \{1,\ldots,n\}$.
\newpage

\begin{customthm}{3}[2023.S(1.A.iii)]
  $ $

  If $T \in Hom(V)$, $W$ is a T-invariant subspace of $V$, and $V = R(T) \oplus W$, then $W \subseteq N(T)$. 

\end{customthm}

\begin{proof}{(Direct Proof)}
  $ $

  Let $w \in W$

  \noindent
  $\implies Tw \in R(T)$ (by definition of the range) and $Tw \in W$ (because W is $T$-invariant)

  \noindent
  $\implies Tw \in R(T) \cap W$.
  \newline

  But since $V = R(T) \oplus W$, then $R(T) \cap W = \{ 0_v \}$

  \noindent
  $\implies Tw = 0_v$

  \noindent
  $\implies w \in N(T)$

  \noindent
  $\implies W \subseteq N(T)$.

\end{proof}

\subsection*{Alternative Proof}
\begin{proof}{(By contradiction)}
  $ $


  Suppose that $W \not\subseteq N(T)$
  
  \noindent
  $\implies \exists w \in W$ s.t. $w \notin N(T)$

  \noindent
  $\implies Tw \in R(T)$ and $Tw \in W$ \quad (Because $W$ is $T$-invariant)

  \noindent
  $\implies R(T) \cap W \neq \{ 0_v \}$

  \noindent
  $\implies V \neq R(T) \oplus W$.
  \newline

  \noindent
  This is a contradiction. Therefore, we must conclude that $W \subseteq N(T)$.

\end{proof}
\newpage

\begin{customthm}{4}[2023.S(1.A.iv)]
  $ $

  Disprove: A linear operator on an infinite-dimensional vector space has no eigenvectors. 

\end{customthm}

\begin{proof}{(By counterexample)}
  $ $

  Consider the vector space $V = \mathbb{R}^\infty$ over $\mathbb{R}$.

  Let $T$ be a linear operator on $V$ defined by $T := \lambda I_\infty$, where $\lambda \in \mathbb{R}$.

  Then, all vectors in $V$ are eigenvectors with eigenvalue $= \lambda$.

  
\end{proof}
\newpage

\begin{customthm}{5}[2023.S(1.A.v)]
  $ $

  If $S$ is a \textbf{\underline{subset}} of an inner product space $V$, then $Span(S)$ is a \textbf{\underline{subspace}} of $(S^\perp)^\perp$.

\end{customthm}

\begin{proof}
  $ $

  We prove a stronger result:  $Span(S) = (S^\perp)^\perp$.
  \newline
  
  We start from the fact (given by another theorem) that if $U$ is a subspace, then $U = (U^\perp)^\perp$. So, now we need only show that $Span(S)^\perp = S^\perp$. This amounts to showing that $(i) Span(S)^\perp \subseteq S^\perp$ and $(ii) S^\perp \subseteq Span(S)^\perp$.
  \newline

  (i) Is true since any vector that is orthgonal to all vectors in $Span(S)$ must also be orthogonal to all vectors in $S$.
  \newline

  (ii) Let $s_o \in S^\perp$ and $s \in Span(S)$

  \noindent
  $\implies s = \sum a_i s_i, \ \forall s_i \in S$

  \noindent
  $\implies \langle s, s_o \rangle = \langle \sum a_i s_i, s_o \rangle = \sum \langle a_i s_i, s_o \rangle = \sum a_i \langle s_i, s_o \rangle = \sum a_i * 0 = 0$

  \noindent
  $\implies s_o \in Span(S)^\perp$

  \noindent
  $\implies S^\perp \subseteq Span(S)^\perp$.

\end{proof}
\newpage

\begin{customthm}{6}[2023.S(1.B), 2022.S(1.A.ii)]
  $ $

  Let $V$ be an n-dimensional vector space and $T \in Hom(V, W)$. Prove that:

  i. $nullity(T) + rank(T) = n$.

  ii. $T$ is injective iff $T$ carries linearly independent subsets of $V$ onto linearly independent subsets of $W$. In other words:

  T is injective $\iff$ If $(v_1,\ldots,v_k)$ are linearly independent, then $(Tv_1,\ldots,Tv_k)$ are linearly independent.

\end{customthm}

\begin{proof}{Part i}
  $ $

  Let $nullity(T) = m$, $0 \leq m \leq n$,  and $B_N = (u_1,\ldots,u_m)$ be a basis for N(T).
  \newline

  Extend $B_N$ to a basis for $V$: $B_V=(u_1,\ldots,u_m,u_{m+1},\ldots,u_n)$.
  \newline

  Let $v \in V$, then $v = \sum_{i=1}^{n} a_i u_i$.

  Apply $T$ to both sides:
  \begin{align*}
    T(v) &= T(\sum_{i=1}^{n} a_i u_i) &&\\
    &= \sum_{i=1}^{n} a_i T(u_i) &&\\
    &= \sum_{i=m+1}^{n} a_i T(u_i) &&\\
  \end{align*}

  This shows that $(Tv_{m+1},\ldots, Tv_n)$ spans $R(T)$.

  Next, we show that it is also linearly independent:
  \begin{align*}
    0_v &= \sum_{i=m+1}^{n} a_i T(u_i) &&\\
    &= T(\sum_{i=m+1}^{n} a_i u_i) &&\\
  \end{align*}

  \noindent
  $\implies \sum_{i=m+1}^{n} a_i u_i \in N(T)$

  \noindent
  $\implies \sum_{i=1}^{m} a_i u_i = \sum_{i=m+1}^{n} a_i u_i$

  \noindent
  $\implies a_i = 0$, for $i=1,...,n$. \qquad (Because $(u_1,\ldots,u_n)$ is linearly independent)

  \noindent
  $\implies (Tu_{m+1},\ldots,Tu_n)$ is linearly independent and hence is a bsis for range $T$.

\end{proof}

\begin{proof}{Part ii}
  $ $

  \textbf{\underline{Forward direction: }} 
  
  Assume $T$ is injective and $(v_1,\ldots, v_n)$ is linearly independent, then:

  \begin{align*}
    0_w &= a_1Tv_1 + \ldots + a_n Tv_n &&\\
    &= T(a_1v_1 + \ldots + a_nv_n)
  \end{align*}

  \noindent
  $\implies a_1v_1 + \ldots + a_nv_n = 0_v$ \qquad (Because T is injective)

  \noindent
  $\implies a1,\ldots,a_n = 0$ \qquad (Because $(v_1, \ldots, v_n)$ is linearly independent)

  \noindent
  $\implies (Tv_1,...,Tv_n)$ is linearly independent.

  \textbf{\underline{Converse direction: }}
  \begin{align*}
    &\text{Let } (v_1,\ldots,v_n) \text{ be a basis for } V \\
    \implies& (v_1,\ldots,v_n) \text { is linearly independent} \\
    \implies& (Tv_1,\ldots,Tv_n) \text { is linearly independent} \\
    \implies& [a_1 Tv_1 + \ldots + a_n Tv_n = 0_w \implies a_1,\ldots,a_n = 0] \\
    \implies& [T(a_1 v_1+\ldots+a_n v_n) = 0_w \implies a_1 v_1 + \ldots + a_n v_n = 0_v] \\
    \implies& N(T) = \{ 0_v \} \\
  \end{align*}

  \noindent
  $\implies T$ is injective.

\end{proof}
\newpage

\begin{customthm}{7}[2023.S(2.A)]
  $ $

  Let $T: P_n(R) \rightarrow R^{n+1}$ be such that:

  \qquad $T(\sum_{i=0}^{n} c_it^i) = (x_0, x_1,\ldots, x_n)$

  where: $x_k = \int_{0}^{1} t^k f(t)dt$ \qquad for $k = 0,\ldots,n$.
  \newline
  
  Show that $T$ is invertible.

\end{customthm}

\begin{proof}
  $ $

  First, we note that the set $B_{P_n(R)} = \{ t^0, \ldots, t^n \}$ is a basis for $P_n(R)$, and $\langle f1, f2 \rangle = \int_{0}^{1} f_1*f_2 \: dt$ defines an inner-product on $P_n(R)$.
  \newline

  It follows that $T$ can be defined equivalently as follows:
  \begin{align*}
    Tf = (\langle t^0, f \rangle, \ldots , \langle t^n, f\rangle)
  \end{align*}

  Finally, we have the following series of implications:
  \begin{align*}
    & Tf = 0_{R^n} \\
    \implies& \forall i \in \{0,\ldots, n\}, \: \langle t^i, f \rangle = 0 \\
    \implies& \sum_{i = 0}^{n} a_i \langle t^i, f \rangle = 0 \\
    \implies& \biggl< \sum_{i = 0}^{n} a_i t^i, f \biggr> = 0 \\
    \implies& \forall f^\prime \in P_n(R), \: \langle f^\prime, f \rangle = 0 \\
    \implies& f = 0 = 0_{P_n(R)} \\
    \implies& T \text{ is injective} \\
    \implies& T \text{ is invertible.} \\
  \end{align*}

\end{proof}
\newpage

\begin{customthm}{8}[2023.S(2.B), 2021.F(2.A)]
  $ $

  Let $V$ and $W$ be n-dimensional vector spaces with order bases $\alpha$ and $\beta$ respectively. If $T$ is an isomorphism from $V$ onto $W$ with $[T]_\alpha^\beta = A$, show that $[T^{-1}]_\beta^\alpha = A^{-1}$

\end{customthm}

\begin{proof}
  $ $

  Let: 
  \begin{align*}
    &[T]_\alpha^\beta = A \\
    &[T^{-1}]_\beta^\alpha = B \\
    &T \alpha_j = \sum\limits_{i = 1}^{n} b_{ij} \beta_i \\
    &T^{-1} \beta_j = \sum\limits_{i = 1}^{n} a_{ij} \alpha_i
  \end{align*}

  Then, we have:
  \begin{align*}
    \alpha_j 
    &= (T^{-1} T) \alpha_j \\
    &= T^{-1} (T \alpha_j) \\
    &= T^{-1} \bigg( \sum\limits_{i = 1}^{n} b_{ij} \beta_i \bigg) \\
    &= \sum\limits_{i = 1}^{n} b_{ij} T^{-1} \beta_i \\
    &= \sum\limits_{i = 1}^{n} b_{ij} \sum\limits_{k = 1}^{n} a_{ki} \alpha_k \\
    &= \sum\limits_{k = 1}^{n} \bigg( \sum\limits_{i = 1}^{n} b_{ij} a_{ki} \bigg) \alpha_k
  \end{align*}

  \noindent
  $\implies \bigg( \sum\limits_{i = 1}^{n} a_{ij} b_{ki} \bigg) = \delta_{kj}$

  \noindent
  $\iff BA = I$

  \noindent
  $\iff B = A^{-1}$.

\end{proof}
\newpage

\begin{customthm}{9}[2023.S(2.C)]
  $ $

  Let $V = M_{2x2}(R)$ and $T(A) = A^t + 2tr(A)I_2$, where $A \in V$ and $A^t$ is the transpose of $A$.

  Find an ordered Basis $\beta$ for $V$ so that $[T]_\beta$ is a diagonal matrix.
\end{customthm}

\begin{proof}
  $ $

  Since $M_{2x2}(R)$ is isomorphic to $R^4$, we'll use $R^4$ in place of $M_{2x2}(R)$.
  \newline

  Let $\alpha$ be the standard basis for $R^4$:
  \begin{align*}
    \alpha = \{ (1, 0, 0, 0)^T , (0, 1, 0, 0)^T, (0, 0, 1, 0)^T, (0, 0, 0, 1)^T\}
  \end{align*}
  
  Then:
  \begin{align*}
    &T(A) = A^t + 2tr(A)I \\ 
    \implies& T = \begin{pmatrix}
      1 & 0 & 0 & 0 \\
      0 & 0 & 1 & 0 \\
      0 & 1 & 0 & 0 \\
      0 & 0 & 0 & 1 \\
    \end{pmatrix}
    +
    2 \begin{pmatrix}
      1 & 0 & 0 & 0 \\
      0 & 0 & 0 & 0 \\
      0 & 0 & 0 & 0 \\
      0 & 0 & 0 & 1 \\
    \end{pmatrix}
    =
    \begin{pmatrix}
      3 & 0 & 0 & 0 \\
      0 & 0 & 1 & 0 \\
      0 & 1 & 0 & 0 \\
      0 & 0 & 0 & 3 \\
    \end{pmatrix}.
  \end{align*}

  Next, to find the eigenvalues of $T$, we solve the characteristic equation:
  \begin{align*}
    &det(T-\lambda I )
    = 0 \\
    \implies& det \begin{pmatrix}
      3-\lambda & 0 & 0 & 0 \\
      0 & -\lambda & 1 & 0 \\
      0 & 1 & -\lambda & 0 \\
      0 & 0 & 0 & 3-\lambda \\
    \end{pmatrix}
    = 0 \\
    \implies& (3-\lambda) * [
      -\lambda (-\lambda (3-\lambda))
      -1 (3-\lambda)
    ]
    = 0 \\
    \implies& (3-\lambda) * [(3-\lambda) (\lambda^2 - 1)] = 0 \\
    \implies& \lambda = 3, \: +1, \: -1.
  \end{align*}


\end{proof}
\newpage


\begin{customthm}{10}[2023.S(3.B)]
  $ $

  Let $V = W \oplus W^\perp$ and $T$ be the projection on $W$ along $W^\perp$.
  
  Show that $T^* = T$.
\end{customthm}

\begin{proof}
  $ $

  Let $u = w_1 + w_2,\ v = w^\prime_1 + w^\prime_2 \ \in V$ where $w_1, w^\prime_1 \in W$ and $w_2, w^\prime_2 \in W^\perp$.

  \begin{align*}
      \langle
        Tu,v
      \rangle 
      &= 
      \langle
        T(w_1 + w_2), w^\prime_1 + w^\prime_2
      \rangle &&\\
      &= 
      \langle
        w_1, w^\prime_1 + w^\prime_2
      \rangle &&\\
      &= 
      \langle
        w_1, w^\prime_1
      \rangle
      +
      \langle
        w_1, w^\prime_2
      \rangle &&\\
      &=
      \langle
        w_1, w^\prime_1
      \rangle
      + 0 &&\\
      &=
      \langle
        w_1, w^\prime_1
      \rangle
      + 
      \langle 
        w_2, w^\prime_1
      \rangle &&\\
      &=
      \langle
        w_1 + w_2, w^\prime_1
      \rangle &&\\
      &= 
      \langle
        w_1 + w_2, T(w^\prime_1 + w^\prime_2)
      \rangle &&\\
      &= 
      \langle
        u, Tv
      \rangle
  \end{align*}

  $\implies T^* = T$.

\end{proof}
\newpage


\begin{customthm}{11}[2023.S(3.C)]
  $ $

  Let $T$ be a linear operator on the inner product space $V$. Show that:

  $\langle
    Tu, Tv
  \rangle 
  =
  \langle
    u, v
  \rangle 
  \quad \forall u,v \in V \iff ||Tu|| = ||u|| \quad \forall u \in V$.
\end{customthm}

\begin{proof}
  $ $

  \textbf{\underline{$\implies$}}:

  Suppose $\langle Tu, Tv \rangle = \langle u, v \rangle$, then:
  \begin{align*}
    ||Tu||
    &= \sqrt{\langle Tu, Tu \rangle} \\
    &= \sqrt{\langle u, u \rangle} \\
    &= ||u||.
  \end{align*}

  \textbf{\underline{$\impliedby$}}:

  \begin{align*}
    & ||Tu||
    = ||u|| &&\\
    \implies & ||Tu||^2
    = ||u||^2 &&\\
    \implies & 
    \langle
      Tu, Tu
    \rangle
    =
    \langle
      u, u
    \rangle &&\\
    \implies & 
    \langle
      u, T^*Tu
    \rangle
    =
    \langle
      u, u
    \rangle &&\\
    \implies & T^*T = I &&\\
    \implies & \langle
      Tu, Tv
    \rangle
    =
    \langle
      u, T^*Tv
    \rangle &&\\
    \implies & \langle
      Tu, Tv
    \rangle
    =
    \langle
      u, Iv
    \rangle 
    =
    \langle
      u, v
    \rangle. &&\\
  \end{align*}

\end{proof}
\newpage


\begin{customthm}{12}[2022.S(1.A.i)]
  $ $

  If $V$ is a vector space and $S_1, S_2 \subseteq V$ with $S_1 \subseteq S_2$, then $S_2^\perp$ is a \textbf{\underline{subspace}} of $S_1^\perp$.
\end{customthm}

\begin{proof}
  $ $

  Since for any $A \subseteq V$, $A^\perp$ is a subspace of $V$, we need only show that: $S^\perp_2$ is a \textbf{{\underline{subset}}} of $S^\perp_1$.

  \begin{align*}
    & \text{Let } s^\perp_2 \in S^\perp_2 &&\\
    \implies & \forall s_2 \in S_2, \:
    \langle
      s^\perp_2, s_2
    \rangle = 0 &&\\
    \implies & \forall s_1 \in (S_2 \cap S_1 = S_1), \:
    \langle
      s^\perp_2, s_1
    \rangle = 0 &&\\
    \implies & S^\perp_2 \subseteq S^\perp_1. &&\\
  \end{align*}


\end{proof}
\newpage


\begin{customthm}{13}[2022.S(1.B)]
  $ $

  Let $V=M_{2x2}(R)$.
  
  i. Show that $V$ has a basis that contains bases for its subspaces $U$ and $W$, where:
  
  \qquad $U = \{ A \in V: A^T = A \}$ and $W = \{ A \in V: A^T = -A \}$.

  ii. Show that $V = U \oplus W$.
\end{customthm}

\begin{proof}
  $ $

  


\end{proof}
\newpage


\begin{customthm}{14}[2022.S(1.D)]
  $ $

  Let $V$ be the vector space of complex numbers over the field $\mathbb{R}$, i.e. $C^1$ over $\mathbb{R}$.

  Let $T: V \rightarrow V$ be defined by $T(z) = \bar{z}$, the complex conjugate of $z$.

  i. Show that $T$ is linear.

  ii. Show that $T$ is not linear if $V$ is redefined to be over the complex field $\mathbb{C}$.
\end{customthm}

\begin{proof}
  $ $

  


\end{proof}
\newpage



\begin{customthm}{15}[2022.S(2.A)]
  $ $

  Let $V$ be an n-dimensional vector space with bases $\alpha = \{\alpha_i\}$ and $\beta = \{\beta_i\}$.
  
  If $P \in Hom(V)$, such that $P(\alpha_i) = \beta_i \quad \forall i$, derive the relation between $[V]_\alpha$ and $[V]_\beta$ for $v \in V$.
\end{customthm}

\begin{proof}
  $ $

  


\end{proof}
\newpage


\begin{customthm}{16}[2022.S(2.B)]
  $ $

  TODO: Reformulate from a problem to a statement.
  \newline

  Let $T: P_2(R) \rightarrow P_2(R)$ be such that: 

  \qquad $T(a + bt + ct^2) = -2b -3c + (a+3b+3c)t + ct^2$.

  i. Find a basis for for the eigenspace $E_1$.

  ii. Is $T$ diagonalizable?

  iii. Is there an operator on $P_2(R)$ whose null space is $E_1$?
\end{customthm}

\begin{proof}
  $ $

  


\end{proof}
\newpage


\begin{customthm}{17}[2022.S(3.B), 2021.F(3.D)]
  $ $

  Let $V$ be an inner product space, and $T \in Hom(V)$.

  i. Show that $N(T^*T)=N(T)$.

  ii. [Prove or Disprove] $rank(T^*T) = rank(T)$.
\end{customthm}

\begin{proof}
  $ $

  


\end{proof}
\newpage


\begin{customthm}{18}[2021.F(1.A.i)]
  $ $

  $V$ is an inner product vector space and $S \subseteq V \ \land \ S \neq \emptyset$ 
  
  \noindent
  $\implies S^\perp$ is a subspace of $V$.
\end{customthm}

\begin{proof}
  $ $

  \textbf{\underline{i. Inclusion of the Zero Vector}}:
  \begin{align*}
   & \forall s \in S, \: 
   \langle
    0_v, s
   \rangle 
  = 0 &&\\
  \implies & 0_v \in S^\perp. &&\\
  \end{align*}


  \textbf{\underline{ii. Closure under Vector Addition}}:
  \begin{align*}
    & \text{Let } s^\perp_1, s^\perp_2 \in S^\perp, \: s \in S &&\\
    \implies & 
    \langle
      s^\perp_1, s
    \rangle
    = 0
    \land
    \langle
      s^\perp_2, s
    \rangle
    = 0 &&\\
    \implies &
    \langle
      s^\perp_1, s
    \rangle
    +
    \langle
      s^\perp_2, s
    \rangle
    = 0 &&\\
    \implies & 
    \langle
      s^\perp_1 + s^\perp_2, s
    \rangle
    = 0 &&\\
    \implies & s^\perp_1 + s^\perp_2 \in S^\perp.
  \end{align*}


  \textbf{\underline{iii. Closure under Scalar Multiplication}}:
  \begin{align*}
    & \text{Let } s^\perp \in S^\perp, \: s \in S, \: k \in F &&\\
    \implies &
    \langle
      s^\perp, s
    \rangle
    = 0 &&\\
    \implies &
    k\langle
      s^\perp, s
    \rangle
    = 0 &&\\
    \implies &
    \langle
      ks^\perp, s
    \rangle
    = 0 &&\\
    \implies & ks^\perp \in S^\perp.
  \end{align*}
  


\end{proof}
\newpage


\begin{customthm}{19}[2021.F(1.A.ii)]
  $ $

  [Disprove] If $T \in Hom(V, W)$, $dim(V)=dim(W)=2$, and $\{ v_1, v_2 \}$ is a basis for $V$, then $\{ T(v_1-v_2), T(v_1)\}$ is a basis for $W$.
\end{customthm}

\begin{proof}
  $ $

  


\end{proof}
\newpage


\begin{customthm}{20}[2021.F(3.B)]
  $ $

  Let $V$ be an inner product space, and $W$ be a finite-dimensional subspace of $V$.
  
  Show that: 
  $w \in W 
  \implies \exists v \in V 
  \ni v \not\in W^\perp \: \land 
  \langle w, v \rangle \neq 0$. 
  \newline

  
  \textbf{\underline{Notes to self:}}

  \qquad 1. I don't think this theorem holds when $W = V$. So I'm going to assume $W \neq V$ in the proof.
  \newline

  \qquad 2. I don't think this theorem holds when $w = 0_v$. So I'm going to assume $w \neq 0_v$ for the proof.
  \newline

  % \qquad 2. This part of the statement: "$v \not\in W^\perp \: \land \langle w, v \rangle \neq 0$" is redundant (both conditions in the logical AND are equivalent).

\end{customthm}

\begin{proof}
  $ $

  Let $W$ be a \textbf{\underline{proper}} subspace of $V$, then $dim(V) = n > dim(W) = m$. 
  
  Let $\{ v_i \}_{i = 1}^{m}$ be an ordered orthonormal basis for $W$, and extend it to an orthonormal basis for $V$: $\{ v_i \}_{i = 1}^{n}$.
  Then, $\{ v_i \}_{i = m+1}^{n}$ is an orthonormal basis for $W^\perp$.
  \newline

  Let $w = \sum_{i = 1}^{m} a_iv_i$. Since $w \neq 0_v$, then at least one $a_i$ is non-zero. Let $k$ be the index of the first non-zero $a_i$.

  Next, let $v = b_kv_k + b_nv_n$, where $a_1, a_n \neq 0$.
  
  Then, $v \notin W^\perp$, and:
  \begin{align*}
    \langle
      w, v
    \rangle 
    &=
    \langle
      w, b_kv_k + b_nv_n
    \rangle &&\\
    &=
    \langle
      w, b_kv_k
    \rangle
    +
    \langle
      w, b_nv_n
    \rangle &&\\
    &=
    \overline{b_k}
    \langle
      w, v_k
    \rangle
    +
    \overline{b_n}
    \langle
      w, v_n
    \rangle &&\\
    &=
    \overline{b_k}
    \langle
      w, v_k
    \rangle
    +
    0 &&\\
    &=
    \overline{b_k}
    \langle
      \sum_{i=k}^{m} a_iv_i, v_k
    \rangle &&\\
    &=
    \overline{b_k}
    \langle
      a_kv_k, v_k
    \rangle
    +
    \overline{b_k}
    \langle
      \sum_{i=k+1}^{m} a_iv_i, v_k
    \rangle &&\\
    &=
    \overline{b_k}a_k
    \langle
      v_k, v_k
    \rangle
    +
    0 &&\\
    &=
    \overline{a_k}a_k \neq 0 &&\\
  \end{align*}

\end{proof}
\newpage


\begin{customthm}{21}[2012.F(1.B)]
  $ $

  Let $V = M_{2x2}(R)$, 
  
  $B \in V$ such that $B=\begin{bmatrix}
    1 & 2 \\
    0 & 3
  \end{bmatrix}
  $, 
  
  $W_1 = \{ A \in V: AB=BA \}$,

  $W_2 = \{ A \in V: A^T = A \}$.


  i. Show that $W_1$ is a subspace of of $V$.

  ii. Find $dim(W_1)$.

  iii. [Prove or Disprove] $V = W_1 \oplus W_2$.
\end{customthm}

\begin{proof}
  $ $

  


\end{proof}
\newpage


\begin{customthm}{22}[2012.F(1.D)]
  $ $

  Let $T_1, T_2 \in Hom(V,W)$.

  Show that: $rank(T_1 + T_2) \leq rank(T_1) + rank(T_2)$.
\end{customthm}

\begin{proof}
  $ $

  


\end{proof}
\newpage


% 

\noindent\rule{\textwidth}{1pt}

\section*{Problems}

\subsection*{1.}[2023.S(3.A)]

Let $V = C([-1, 1])$ with the inner product $\langle f,g \rangle = \int_{-1}^{1} f(t)g(t)dt \enspace \forall f,g \in V$.

i. Find an orthonormal basis for $P_2(R)$ as a subspace of $V$ and use it to compute the best quadratic approximation of $f(t) = e^t$ on $[-1, 1]$.

ii. For $T \in Hom(P_1(R))$ with $P_1(R)$ as a subspace of $V$ and $T(f) = f^\prime + 3f$, evaluate $T^*(1 + 3t)$.
\newline

\begin{center}
  \textbf{\underline{Solution:}}  
\end{center}

i. The ordered set $\alpha = \{a_i \}_{i = 0}^{2} = \{t^i\}_{i = 0}^{2}$ is a basis for $P_2(R)$.
We use the Gram-Schmidt process to transform $\alpha$ to an orthonormal basis $\beta= \{b_i \}_{i = 0}^{2}$.
\newline

\underline{$b_1$}:

$||a_1||^2 
= \langle a_1, a_1 \rangle 
= \langle 1, 1 \rangle
= \int_{-1}^{1} 1*1 dt 
= [t]_{-1}^{1} = 2$.
\newline

$b_1 = \frac{a_1}{||a_1||} 
= \frac{1}{\sqrt{2}}$.
\newline


\underline{$b_2$}:

$a_2 = t$.
\newline

$\langle a_2, b_1\rangle 
= \int_{-1}^{1} t*\frac{1}{\sqrt{2}} dt 
= [\frac{t^2}{2\sqrt{2}}]_{-1}^{1} = 0$.
\newline

$a_2 - \langle a_2, b_1\rangle b_1 = t$.
\newline

$||a_2 - \langle a_2, b_1\rangle b_1||^2 
= ||t||^2 = \langle t, t \rangle 
= \int_{-1}^{1} t^2 dt 
= [\frac{t^3}{3}]_{-1}^{1} 
= \frac{2}{3}$.
\newline

$b_2 = \frac{a_2 - \langle a_2, b_1\rangle b_1}{||a_2 - \langle a_2, b_1\rangle b_1||} 
= \sqrt{\frac{3}{2}}t$
\newline


\underline{$b_3$}:

$a_3 = t^2$.
\newline

$\langle a_3, b_1\rangle = \int_{-1}^{1} t^2*\frac{1}{\sqrt{2}} dt 
= [\frac{t^3}{3\sqrt{2}}]_{-1}^{1} 
= \frac{\sqrt{2}}{3}$.
\newline

$\langle a_3, b_2\rangle 
= \int_{-1}^{1} t^2*\sqrt{\frac{3}{2}}t dt 
= [\frac{t^4}{4\sqrt{2}}]_{-1}^{1} 
= 0$.
\newline

$a_3 - \langle a_3, b_1\rangle b_1 - \langle a_3, b_2\rangle b_2 
= t^2 - \frac{\sqrt{2}}{3} * \frac{1}{\sqrt{2}}
= t^2 - \frac{1}{3}$.
\newline

$||a_3 - \langle a_3, b_1\rangle b_1 - \langle a_3, b_2\rangle b_2||^2
= ||t^2 - \frac{1}{3}||^2
= \langle t^2 - \frac{1}{3}, t^2 - \frac{1}{3}\rangle
= \int_{-1}^{1} (t^2 - \frac{1}{3})*(t^2 - \frac{1}{3}) dt
= \int_{-1}^{1} (t^4 - \frac{2}{3} t^2 + \frac{1}{9}) dt
= [\frac{t^5}{5} - \frac{2}{9} t^3 + \frac{1}{9} t]_{-1}^{1}
= \frac{8}{45}$.
\newline

$b_3 = \frac{a_3 - \langle a_3, b_1\rangle b_1 - \langle a_3, b_2\rangle b_2}{||a_3 - \langle a_3, b_1\rangle b_1 - \langle a_3, b_2\rangle b_2||}
= \frac{t^2 - \frac{1}{3}}{\sqrt{\frac{8}{45}}}
= \sqrt{\frac{45}{8}} t^2 - \sqrt{\frac{5}{8}}$.
\newline

\underline{Best quadratic approximation of $f(t)=e^t$:}

The best approximation is given by: 
$P_{P_2}(f(t)) 
= 
\langle f(t), b_1 \rangle b_1
+
\langle f(t), b_2 \rangle b_2
+
\langle f(t), b_3 \rangle b_3
$.
\newline

$\langle f(t), b_1 \rangle
= \langle e^t, \frac{1}{\sqrt{2}} \rangle
= \int_{-1}^{1} \frac{1}{\sqrt{2}} e^t dt
= [\frac{1}{\sqrt{2}} e^t]_{-1}^{1}
= 1.662.
$
\newline

$\langle f(t), b_2 \rangle
= \langle e^t, \sqrt{\frac{3}{2}}t \rangle
= \int_{-1}^{1} e^t * \sqrt{\frac{3}{2}}t dt
= 0.901.
$
\newline

$\langle f(t), b_3 \rangle
= \langle e^t, \sqrt{\frac{45}{8}} t^2 - \sqrt{\frac{5}{8}} \rangle
= \int_{-1}^{1} e^t * (\sqrt{\frac{45}{8}} t^2 - \sqrt{\frac{5}{8}}) dt
= 0.226.
$
\newline

$\implies f(t) 
\approx 
0.226(\sqrt{\frac{45}{8}} t^2 - \sqrt{\frac{5}{8}})
+
0.901 * \sqrt{\frac{3}{2}}t
+
\frac{1.662}{\sqrt{2}}
=
0.536t^2 + 1.103t + 0.997$
\newline


ii. Let $v=a_0b_1 + a_1b_2 = (a_0, a_1), w=a_0^\prime b_1 + a_1^\prime b_2 = (a_0^\prime, a_1^\prime) \in P_1(R)$.

\begin{align*}
  \langle
    v, T^*w
  \rangle
  &=
  \langle
    Tv, w
  \rangle &&\\
  &= 
  \langle
    T(a_0, a_1), (a_0^\prime, a_1^\prime)
  \rangle &&\\
  &= 
  \langle
    (3a_0 + a_1, 3a_1), (a_0^\prime, a_1^\prime)
  \rangle &&\\
  &= 
  \langle
    (3a_0 + a_1, 0), (a_0^\prime, 0)
  \rangle 
  +
  \langle
    (0, 3a_1), (0, a_1^\prime)
  \rangle 
  &&\\
  &= 
  \langle
    (3a_0, 0), (a_0^\prime, 0)
  \rangle 
  +
  \langle
    (a_1, 0), (a_0^\prime, 0)
  \rangle 
  +
  \langle
    (0, 3a_1), (0, a_1^\prime)
  \rangle 
  &&\\
  &= 
  \langle
    (a_0, a_1), (3a_0^\prime, a_0^\prime + 3a_1^\prime)
  \rangle &&\\
\end{align*}

$\implies T^*(a_0, a_1) = (3a_0, a_0 + 3a_1)$.
\newline

Let $v = 1+3t$, then:

\qquad $\langle v, b_1 \rangle = \sqrt{2}$.

\qquad $\langle v, b_2 \rangle = 2.45$.

Therefore, $v = \sqrt{2}b_1 + 2.45b_2 = (\sqrt{2}, 2.45)$.
\newline

$\implies T^*v = (3\sqrt{2}, \sqrt{2} + 3*2.45) = (4.24, 8.76)$.
\newpage


\section*{2.}[2022.S(1.C), 2021.F(1.B)]

Let $V=R^3$.

i. Suggest two 2-dimensional subspaces $W_1$ and $W_2$ of $V$ such that $V=W_1 + W_2$.

ii. With $W_1= \{v \in V: v=(x,y,0) \}$, define two projections \textbf{\underline{on}} $W_1$ \textbf{\underline{along}} two distinct subspaces $W_2$ amd $W_3$ of $V$.

\newpage


\section*{3.}[2022.S(3.A), 2021.F(3.A)]

Let $V = P_1(R) = Span(\{ 1,t \})$ with the inner product $\langle f,g \rangle = \int_{0}^{1} f(t)g(t)dt, \: \forall f,g \in V$.

  i. Find an orthonormal basis for V.
  
  ii. Find the orthogonal projection of $f(t)= t^2$ on $V$.

  iii. Let $T(f) = f^\prime(t) + 3f(t)$ be a linear operator on $V$. Is $(t-2)$ an eigenvector for $T$?

  iv. Evaluate $T^*(2t-1)$.


  \begin{center}
    \textbf{\underline{Solution:}}  
  \end{center}

  i. Let $\alpha = \{ a_i \} = \{ t^i \}$ be the standard basis.
  We use the Gram-Schmidt process to orthonormalize this basis to obtain $E=\{e_i\}$.
  \newline

  \underline{$e_1$:}

  $||a_1||^2 
  = \langle 1, 1\rangle 
  = 1$.
  \newline

  $e_1 = \frac{a1}{||a_1||} 
  = \frac{1}{\sqrt{1}} 
  = 1$.
  \newline

  \underline{$e_2$:}

  $\langle a_2, e_1\rangle 
  = \langle t, 1\rangle 
  = \frac{1}{2}$.
  \newline

  $a_2 - \langle a_2, e_1\rangle e_1 
  = t - \frac{1}{2}$.
  \newline

  $||a_2 - \langle a_2, e_1\rangle e_1||^2
  = ||t - \frac{1}{2}||^2
  = \langle t - \frac{1}{2}, t - \frac{1}{2}\rangle
  = \frac{1}{12}$.
  \newline

  $e_2 
  = \frac{a_2 - \langle a_2, e_1\rangle e_1}{||a_2 - \langle a_2, e_1\rangle e_1||}
  = \frac{t - \frac{1}{2}}{\sqrt{\frac{1}{12}}}
  = \sqrt{12}(t - \frac{1}{2})$.
  \newline


  ii. 

  $\langle f(t), e_1\rangle
  =
  \langle t^2, 1\rangle
  =
  0.333$.
  \newline

  $\langle f(t), e_2\rangle
  =
  \langle t^2, \sqrt{12}(t - \frac{1}{2})\rangle
  =
  0.289$.
  \newline

  $\implies f(t) 
  = e^t 
  \approx 
  0.333 + 0.289*\sqrt{12}(t - \frac{1}{2})
  = -0.168 + 1.001t$.
  \newline


  iii. 
  $T(t-2)
  = 1 + 3(t-2)
  = -5 + 3t
  \neq \lambda (t-2)$.

  $\implies (t-2)$ is not an eigenvector of $T$.
  \newline

  iv. Let $v = a_1 e_1 + a_2 e_2 = (a_1, a_2), w= b_1 e_1 + b_2 e_2 = (b_1, b_2) \in P_1(R)$

  \begin{align*}
    \langle
      v, T^*w
    \rangle
    &=
    \langle
      Tv, w
    \rangle &&\\
    &=
    \langle
      T(a_1, a_2), (b_1, b_2)
    \rangle &&\\
    &=
    \langle
      (3a_1 + a_2, 3a_2), (b_1, b_2)
    \rangle &&\\
    &=
    \langle
      (3a_1 + a_2, 0), (b_1, 0)
    \rangle 
    +
    \langle
      (0, 3a_2), (0, b_2)
    \rangle &&\\
    &=
    \langle
      (3a_1, 0), (b_1, 0)
    \rangle 
    +
    \langle
      (a_1, 0), (b_1, 0)
    \rangle 
    +
    \langle
      (0, 3a_2), (0, b_2)
    \rangle &&\\
    &=
    3a_1b_1 + a2b_1 + 3a_2b_2 &&\\
    &=
    \langle
      (a_1, a_2), (3b_1, b_1 + 3b_2)
    \rangle. &&\\
  \end{align*}
  
  $\implies T^*(c_1, c_2) = (3c_1, c_1 + 3c_2)$.
  \newline

  Let $u = 2t-1$.

  $u 
  = \langle u, e_1 \rangle e_1  + \langle u, e_2 \rangle e_2
  = 0.577 e_2$.

  $T^*u 
  = (0, 1.731)
  $ .

  \newpage


  \section*{4.}[2021.F(1.C)]

  Let $T: P_2(R) \rightarrow R^2$ be such that $T(a_0 + a_1t + a_2t^2) = (a_1+a_2, a_0-a_1)$.

  i. Find a basis for $ker(T)$.
  ii. Is $T$ surjective?
  iii. Find a two-dimensional subspace of of $P_2(R)$ such that its image under $T$ is a one-dimensional subspace of $R^3$.
  iv. Find the matrix representation of $T$ relative to $\{1, t-1, t^2+1 \}$ as a basis for $P_2(R)$ 
  and $\{(0,1), (1,1)\}$ as a basis for $R^2$.

  \newpage


  \section*{5.}[2021.F(2.B)]

  $T$ is a linear operator on $P_2(R)$ defined by:
  \qquad $T(f(x)=xf^\prime(x) + f(2) + f(3))$.

  i. Is $T$ diagonalizable?

  ii. Find an eigenpair for $T$.

  \newpage


  \section*{6.}[2021.F(3.C)]

  Let $V = \mathbb{C}^2$ and $T(z_1, z_2) = (2z_1 + iz_2, (1-i)z_1)$. 
  
  Evaluate $T^*(3-i, 1+2i)$.
  
  
  \begin{center}
    \textbf{\underline{Solution:}}  
  \end{center}

  Let $w = (w_1, w_2), z = (z_1, z_2) \in C^2$.

  \begin{align*}
    \langle
      w, T^*z
    \rangle
    &=
    \langle
      Tw, z
    \rangle &&\\
    &=
    \langle
      T(w_1, w_2), (z_1, z_2)
    \rangle &&\\
    &=
    \langle
      (2w_1 + iw_2, (1-i)w_1), (z_1, z_2)
    \rangle &&\\
    &=
    (2w_1 + iw_2)z_1 + (1-i)w_1z_2 &&\\
    &=
    w_1(2z_1 + (1-i)z_2) + w_2(iz_1) &&\\
    &=
    \langle
      (w_1, w_2), (2z_1 + (1-i)z_2, iz_1)
    \rangle
  \end{align*}

  $\implies T^*(z_1, z_2) = (2z_1 + (1-i)z_2,\: iz_1)$.

  Let $v = (3-i, 1+2i)$, then 
  $T^*v 
  = (2(3-i) + (1-i)(1+2i), i(3-i))
  = (...,\: ...)$.
  
  \newpage


  \section*{7.}[2012.F(4.B)]

  Find the minimal $l_2$-norm solution to the system:

  \begin{align*}
    x + 2y  - z &= 1 &&\\
    2x + 3y + z &= 2 &&\\
    4x + 7y - z &= 4.
  \end{align*}

  % DRAFT
  \newpage
  \begin{proof}
    \begin{align*}
      &
      \begin{pmatrix}
        1 & -1 & 1 & 1 \\
        0 & 1 & 2  & 1 \\
        -1 & 2 & 1  & 0 \\
        2 & 0 & 6 & 4
      \end{pmatrix} \\
      \rightarrow& 
      \begin{pmatrix}
        1 & -1 & 1 & 1 \\
        0 & 1 & 2  & 1 \\
        0 & 1 & 2  & 1 \\
        0 & 2 & 4 & 2
      \end{pmatrix} \\
      \rightarrow& 
      \begin{pmatrix}
        1 & 0 & 3 & 2 \\
        0 & 1 & 2  & 1 \\
        0 & 0 & 0  & 0 \\
        0 & 0 & 0 & 0
      \end{pmatrix} \\
    \end{align*}
  \end{proof}
\end{document}
\documentclass{article}
\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{amsthm}
\usepackage{centernot}
\author{Mostafa Hassanein}
\title{Functional Analysis Assignment (Chapter 1)}
\date{16 March 2024}
\begin{document}

\maketitle

\newpage

\section*{1.1.1}

The distance on $\mathbb{R}$ is defined by $d(x,y)=|x-y|$. We must check that the 4 axioms (M1 to M4) are satisfied.
\newline

M1 holds, since the absolute value of the difference between 2 real points is real, finite, and non-negative.
\newline

M2 holds, since $d(x,y) = |x-y| = 0 \iff x = y$. 
\newline

M3 holds, since $d(x,y) = |x-y| = |y-x| = d(y,x)$.
\newline

M4 holds, since the triangle inequality holds for the absolute value.


\section*{1.1.2}
$d(x,y) = (x-y)^2$ is not a valid metric since it does not satisfy the triangle inequality.

\begin{proof}{(By Counterexample)}
  $ $

  Let $a,b,c \in R$, where $a = 0, b = 1, c = 5$, then:
  \newline
  
  $d(a,c) = (a - c)^2 = (0 - 5)^2 = 25$.

  $d(a,b) = (a - b)^2 = (0 - 1)^2 = 1$.

  $d(b,c) = (b - c)^2 = (1 - 5)^2 = 16$.

  $d(a,b) + d(b,c) = 1 + 16 = 17$.
  \newline

  Therefore, 
  $d(a,c) \not \leq d(a,b) + d(b,c)$.

\end{proof}


\section*{1.1.3}

Since the distance function $d(x,y) = |x-y|$ defines a metric on $\mathbb{R}$, as shown in 1.1.1, then it is clear that the square root of that metric is also real, finite, and non-negative (i.e. M1 holds); definite (i.e. M2 holds); and symmetric (i.e. M3 holds).
\newline

\noindent
The triangle inequality can be shown to hold by noting that the square root function is an increasing function with a negative second derivative in the interval $(0, \infty)$.
\newline

\noindent
This shows that $d(x,y) = \sqrt{|x-y|}$ is a metric on $\mathbb{R}$.


\section*{1.1.4}

\subsection*{i. $|X| = 2$}
Let $X = \{a,b\}$, then $d$ must satisfy:
\newline

$d(a,a) = d(b,b) = 0$, and


$d(a,b) = d(b,a) = c$, where $c$ is any non-negative real number.

\subsection*{ii. $|X| = 1$}
In this case the only valid metric is $d(a,a) = 0$.


\section*{1.1.5}
\subsection*{i. Conditions for $kd$ to be a metric}
If $d$ is a metric, then $kd$ automatically satisfies axioms M2-M4.
\newline

\noindent
For axiom M1 to hold, $k$ must be a non-negative real number.

\subsection*{ii. Conditions for $k+d$ to be a metric}
To satisfy axiom M2, $k$ must be zero.


\section*{1.1.6}

\begin{proof}{(By Induction on the Length of the Sequence)}
  $ $

  Let $X=(x_j), \ Y=(y_j), \ Z=(z_j)$ be 3 bounded sequences.
  \newline

  \textbf{\underline{Base case}}

  Consider the subsequence of $X,Y,Z$ consisting of just their first element. Then by the triangle inequality for numbers:

  \qquad $|x_1 - z_1| \leq |x_1 - y_1| + |y_1 - z_1|$

  $\implies \sup |x_1 - z_1| \leq \sup (|x_1 - y_1| + |y_1 - z_1|)$

  $\implies \sup |x_1 - z_1| \leq \sup |x_1 - y_1| + \sup |y_1 - z_1|$.
\newline

  Therefore $d(x,z) \leq d(x,y) + d(y,z)$ holds for sequences of length 1.
  \newline

\textbf{\underline{Inductive step}}

Next, we'll consider the sub-sequences of $X,Y,Z$ consisting of the first $n+1$ elements. Suppose that the induction hypothesis holds for sequences of length $n$, i.e.:

\qquad $\sup\limits_{j \in \{1..n\}} |x_j - z_j| \leq \sup\limits_{j \in \{1..n\}} |x_j - y_j| + \sup\limits_{j \in \{1..n\}} |y_j - z_j|$.
\newline

Then we can partition each sequence of length $n+1$ into 2 sub-sequences: the first sequence contains the first $n$ elements and the second contains the last element.
\newline

The distance between any 2 sequences of length $n+1$ then becomes:

\qquad$\max(\sup \limits_{j \in 1..n} |x_j - z_j|, \ \sup |x_{n+1} - z_{n+1}|)$
\newline

Finally, applying the induction hypothesis we get:

\qquad$\max(\sup \limits_{j \in 1..n} |x_j - z_j|, \ \sup |x_{n+1} - z_{n+1}|) \leq \max(\sup\limits_{j \in 1..n} |x_j-y_j| + \sup\limits_{j \in 1..n} |y_j-z_j|, \ \sup |x_{n+1} - y_{n+1}| + \sup |y_{n+1} - z_{n+1}|)$
\newline

$\implies \sup\limits_{j \in 1..n+1} |x_j - z_j| \leq \sup \limits_{j \in 1..n+1} |x_j-y_j| + \sup \limits_{j \in 1..n+1} |y_j-z_j|$
\newline

Therefore $d(x,z) \leq d(x,y) + d(y,z)$ holds for sequences of any length.

\end{proof}

\section*{1.1.7}

$d(x,y) = \begin{cases}
  0 & x=y \\
  1 & x\not=y
\end{cases}$
\newline

\noindent
Which is the discrete metric.

\section*{1.1.8}

M1 holds because $|x(t) - y(t)|$ is a positive function, and the integral of a positive function is positive.
\newline

\noindent
M2 holds because $|x(t) - x(t)| = 0$, and the integral of the zero function is 0.
\newline

\noindent
M3 holds because $d(x, y) = \int_{a}^{b} |x(t) - y(t)| dt = \int_{a}^{b}|y(t) - x(t)| dt = d(y, x)$.
\newline

\noindent
To show that M4 holds, let $x, y, z \in X$, then:
\begin{align*}
  d(x,z) 
  &= \int_{a}^{b} |x(t) - z(t)| \ dt &&\\
  &= \int_{a}^{b} |(x(t) - y(t)) - (z(t) - y(t))| \ dt &&\\
  &\geq \int_{a}^{b} (|x(t) - y(t)| - |z(t) - y(t)|) \ dt \qquad \text{(By the triangle inequality of absolute values)} &&\\
  % &= \int_{a}^{b} (|x(t) - y(t)| - |y(t) - z(t)|) \ dt &&\\
  &= \int_{a}^{b} |x(t) - y(t)| dt - \int_{a}^{b} |z(t) - y(t)| dt &&\\
  &= d(x, y) - d(z, y) &&\\
\end{align*}

\noindent
$\implies d(x, y) \leq d(x, z) + d(z, y).$

\newpage

\section*{1.2.3}
\begin{proof}
  $ $
  
  Let $(\zeta_j)$, $(\eta_j)$, and $(\theta_j) \in l^p$, where $(\zeta_j)$ is any point, and define $(\eta_j)$ and $(\theta_j)$ as follows:

  $\eta_j := \begin{cases}
    1 & j \leq n \\
    0 & j > n
  \end{cases}$

  $\theta_j := \begin{cases}
    \zeta_j & j \leq n \\
    0 & j > n
  \end{cases}$
  \newline

  \noindent
  Applying the Cauchy-Schwarz inequality to $(\theta_j)$ and $(\eta_j)$, we get:

  \begin{align*}
    \sum_{j=1}^{\infty} |\theta_j \eta_j| 
    &\leq \sqrt{\sum_{k=1}^{\infty} |\theta_j|^2} \sqrt{\sum_{m=1}^{\infty} |\eta_j|^2} &&\\
    \sum_{j=1}^{n} |\theta_j \eta_j| 
    &\leq \sqrt{\sum_{k=1}^{n} |\theta_j|^2} \sqrt{\sum_{m=1}^{n} |\eta_j|^2} &&\\
    \sum_{j=1}^{n} |\theta_j| 
    &\leq \sqrt{\sum_{k=1}^{n} |\theta_j|^2} \sqrt{\sum_{m=1}^{n} 1} &&\\
    \sum_{j=1}^{n} |\zeta_j| 
    &\leq \sqrt{\sum_{k=1}^{n} |\zeta_j|^2} \sqrt{n} &&\\
    \left( \sum_{j=1}^{n} |\zeta_j| \right)^2
    &\leq n \sum_{k=1}^{n} |\zeta_j^2| &&\\
  \end{align*}
\end{proof}

\section*{1.2.4}

% Let $\zeta_j := \frac{1}{\ln(j + 1)}$.
% \newline

% \noindent
% Clearly the sequence $(\zeta_j)$ converges to zero as can be verified by evaluating the limit as j goes to infinity using L'Hopital rule.
% \newline

% \noindent
% Now we show that the series $\sum_{j=1}^{\infty} |\zeta_j|^p$ is divergent. We show this using the comparison test.

% \noindent
% Let $\eta_j = \frac{1}{j + 1}$.

% \noindent
% Since $\frac{1}{j + 1} \leq $

\section*{1.2.5}

\begin{proof}
  $ $
  
  The sequence $\zeta_j = \frac{1}{j}$ is divergent for $p=1$ but convergent for all $p > 1$.

\end{proof}

\section*{1.2.11}

\begin{proof}
  $ $

  M1 holds because: 
  \begin{align*}
    &d(x,y) \geq 0 &&\\
    \implies& \frac{d(x,y)}{1+d(x,y)} \geq 0 &&\\
    \implies& \tilde{d}(x,y) \geq 0.
  \end{align*}

  M2 holds because:

  \textbf{\underline{$\implies$}}:
  \begin{align*}
    &\tilde{d}(x,y) = 0 &&\\
    \implies& \frac{d(x,y)}{1+d(x,y)} = 0 &&\\
    \implies& d(x,y) = 0 &&\\
    \implies& x = y. &&\\
  \end{align*}

  \textbf{\underline{$\impliedby$}}:
  \begin{align*}
    &x = y &&\\
    \implies& d(x,y) = 0 &&\\
    \implies& \frac{d(x,y)}{1 + d(x,y)} = 0 &&\\
    \implies& \tilde{d}(x,y) = 0.
  \end{align*}

  M3 holds because: 
  \begin{align*}
    &d(x,y) = d(y,x) &&\\
    \implies& \frac{d(x,y)}{1+d(x,y)} = \frac{d(y,x)}{1+d(y,x)} &&\\
    \implies& \tilde{d}(x,y) = \tilde{d}(y,x).
  \end{align*}

  M4 holds because:
  \begin{alignat*}{2}
    &d(x,z) &&\leq d(x,y) + d(y,z) \\
    \implies& \frac{d(x,z)}{1+d(x,z)} &&\leq \frac{d(x,y)}{1+d(x,z)} + \frac{d(y,z)}{1 + d(x, z)} \\
    \implies& \frac{d(x,z)}{1+d(x,z)} &&\leq \frac{d(x,y)}{1+d(x,y)} + \frac{d(y,z)}{1 + d(y,z)} \\
    \implies& \tilde{d}(x,z) &&\leq \tilde{d}(x,y) + \tilde{d}(y,z).
  \end{alignat*}
\end{proof}

\newpage

\section*{1.3.4}
\begin{proof}
  $ $

  \noindent
  \textbf{\underline{$\implies$ (By Construction)}}:

  $ $
  
  Let $A$ be a non-empty open subset, and $\delta(A)$ be the diameter of A.

  Since $A$ is non-empty and open, then $|A| > 1$.

  Let $a_1, \: a_2$ be 2 different points in $A$.

  Consider the 2 open balls $B(a_1, \delta(A))$ and $B(a_2, \delta(A))$.

  We have $B(a_1, \delta(A)) = B(a_2, \delta(A)) = A$.

  Thus, $A = B(a_1, \delta(A)) \cup B(a_2, \delta(A))$.
  \newline
  
  \noindent
  \textbf{\underline{$\impliedby$ (By Strong Induction)}}:

  $ $

  We will prove a \textbf{\underline{stronger}} result: The union of non-empty open subsets is a non-empty open subset.

  Let $B=(B_1, B_2, \ldots, B_n)$ be a list of open subsets (balls \textbf{\underline{or not}}) in $X$.
  \newline 

  \underline{Base case: $|B| = 2$}

  \begin{align*}
    &A = B_1 \cup B_2 \\
    \implies& \forall a \in A \:, \: a \in B_1 \lor a \in B_2 \\
    \implies& \forall a \in A, \: \exists \text{ a ball about } a \\
    \implies& A \text{ is a non-empty open subset of } X.
  \end{align*}
  
  \underline{Inductive step: $|B|= k > 2$}

  \begin{align}
    A &= \bigcup_{j = 1}^{k} B_j \nonumber \\
    &= \bigcup_{j = 1}^{k-1} B_j + B_n \nonumber \\
    &= C + B_k
  \end{align}
  
  \noindent
  Where $C$ is a non-empty open subset by the induction hypothesis for $n = k-1$.
  \newline

  \noindent
  Applying the induction hypothesis to (1) for $n = 2$, we conclude that $A$ is a non-empty open subset in $X$.

\end{proof}

\section*{1.3.6}

\begin{proof}(By Construction)
  $ $

  We will prove this statement by recursively (and infinitely) applying the definition of an accumulation point on increasingly smaller $\epsilon$-neighborhoods.
  \newline

  Let the sequence of $(\epsilon_j)$ be such that $\forall j, \: \epsilon_j > 0$.
  \newline

  Since $x_0$ is an accumulation point,
  then each $\epsilon_j$-neighborhood contains at least one point $y \in A$.
  \newline
  
  Let $y_j$ to be any point in the $\epsilon_j$-neighborhood of $x_0$ that satisfies the following condition:
  \begin{center}
    $d(y_j, x_0) = \max\limits_{\forall y_k \in \tilde{B}(x_0; \epsilon_j)} d(y_k, x_0)$
  \end{center}

  Define $\epsilon_j$ recursively as follows:
  
  \begin{center}
    $\epsilon_j := \begin{cases}
      \epsilon_1 & j = 1 \\
      d(x_0, y_{j-1})/2 & j > 1
    \end{cases}$
  \end{center}

  Then the sequence $(y_j)$ is an infinite sequence of distinct points in A, all contained inside the $\epsilon_1$-neighborhood of $x_0$. 
  \newline
  
  Since $\epsilon_1$ was arbitrarily chosen, this completes the proof.

\end{proof}

\section*{1.3.13}
\begin{proof}
  $ $

  This follows directly from the definition that a separable space $X$ contains a subset $Y$ that is countable and dense in $X$.

\end{proof}

\section*{1.3.14}
\begin{proof}
  $ $
  
  \underline{$\implies$}:

  Let $T: X \longrightarrow Y$ be a continuous map and $M$ be a closed subset in $Y$.
  \begin{alignat*}{2}
    \implies& M^c \text{ is open} \quad &&\text{(Because the complement of a closed set is an open set by definition 1.3-2)} \\
    \implies& T^{-1}(M^c) \text{ is open } \quad &&\text{(By continuity of T)} \\
    \implies& (T^{-1}(M^c))^c \text{ is closed } \quad &&\text{(Because the complement of an open set is a closed set by definition 1.3-2)} \\
    \implies& T^{-1}((M^c)^c) \text{ is closed } \quad &&\text{(By the identity: } f^{-1}(A^c) = [f^{-1}(A)]^c \text{)} \\
    \implies& T^{-1}(M) \text{ is open.} \quad &&\text{(By the identity: } A = [A^c]^c \text{)}\\
  \end{alignat*}

  \underline{$\impliedby$}:

  Let $T: X \longrightarrow Y$ be a map such that the inverse image of any closed set in $Y$ is a closed set in $X$.
  
  Let $A$ be an open set in $Y$.
  \begin{alignat*}{2}
    \implies& A^c \text{ is closed} \quad &&\text{(Because the complement of an open set is a closed set by definition 1.3-2)} \\
    \implies& T(A^c) \text{ is closed} \quad &&\text{(By our assumption the inverse image of any closed set is a closed set)} \\
    \implies& [T(A^c)]^c \text{ is open} \quad &&\text{(Because the complement of a closed set is an open set by definition 1.3-2)} \\
    \implies& T([A^c]^c) \text{ is open} \quad &&\text{(By the identity: } f^{-1}(A^c) = [f^{-1}(A)]^c \text{)} \\
    \implies& T(A) \text{ is open} \quad &&\text{(By the identity: } A = [A^c]^c \text{)} \\
    \implies& T \text{ is continuous.}
  \end{alignat*}

\end{proof}

\newpage

\section*{1.4.2}
\begin{proof}
  $ $

  \begin{align*}
    & (x_{n_k}) \text{ converges to x} \\
    \implies& \forall \epsilon > 0, \: \exists K(\epsilon) \ni \forall k > K, \: d(x_{n_k}, x) < \epsilon \\
    \implies& \forall \epsilon > 0, \: \exists N(\epsilon) = K(\epsilon) \ni \forall n > N, \: d(x_n, x) < \epsilon \quad \text{(Because $(x_n)$ is Cauchy)} \\
    \implies& (x_n) \text{ converges to } x.
  \end{align*} 

\end{proof}

\section*{1.4.4}
\begin{proof}
  $ $

  Take $\epsilon$ with any concrete value, say $\epsilon = 1$.
  \newline

  Because $(x_n)$ is Cauchy, then:
  \begin{align*}
    \exists N \ni \forall n,m > N, d(x_n, x_m) < \epsilon = 1. 
  \end{align*}

  Define $a = \max\limits_{\forall i,j \in \{1, \ldots, N\}} d(x_i, x_j)$.
  \newline

  Therefore we have $\forall n$:
  \begin{align*}
    d(x_n, x_N) \leq \max(1, a) \leq 1 + a
  \end{align*}

  By the triangle inequality we have $\forall n, m$:
  \begin{align*}
    d(x_n, d_m) &\leq d(x_n, x_N) + d(x_N, x_m) \\
    &\leq (1+a) + (1+a) = 2(1+a) = u.
  \end{align*}
  
  This shows that $u$ is an upperbound for the Cauchy sequence.

\end{proof}


\section*{1.4.5}
\begin{proof}(By Counterexample)
  $ $

  \underline{i. Boundedness does \textbf{not} imply Cauchiness}:

  Consider the sequence $x_n = (-1)^n$.

  $(x_n)$ is bounded by $2$, but the sequence is not Cauchy, because for any $0 < \epsilon < 2$: 
  \begin{align*}
    \not \exists N \ni \forall n,m > N, \: d(x_n, x_m) < \epsilon.
  \end{align*}

  \underline{ii. Boundedness does \textbf{not} imply Convergence}:

  Consider the sequence $x_n = sin(n)$.

  $(x_n)$ is bounded by $2$, but the sequence oscillates and does not converge.

\end{proof}

\section*{1.4.6}
\begin{proof}
  $ $

  Take any $\epsilon > 0$, then since $(x_n)$ and $(y_n)$ are Cauchy:
  \begin{align*}
    \exists N_x \ni \forall n,m > N, \: d(x_n, x_m) < \epsilon
  \end{align*}
  and,
  \begin{align*}
    \exists N_y \ni \forall n,m > N, \: d(y_n, y_m) < \epsilon.
  \end{align*}

  Let $N = \max \lbrace N_x, N_y \rbrace$, then $\forall n > N$:
  \begin{align*}
    d(x_n, x_N) < \epsilon \: \land \: d(y_n, y_N) < \epsilon.
  \end{align*}

  By the triangle inequality we have $\forall n > N$:
  \begin{align*}
    a_n = d(x_n, y_n) &\leq d(x_n, x_N) + d(x_N, y_N) + d(y_N, y_n) \\
    &\leq \epsilon + c + \epsilon \\
    &=  2\epsilon + c.
  \end{align*}

  Finally, $\forall n,m > N$:
  \begin{align*}
    d(a_n, a_m) &\leq d(a_n, a_N) + d(a_N, a_m) \\
    &= |d(x_n, y_n) - d(x_N, y_N)| + |d(x_m, y_m) - d(x_N, y_N)| \\
    &\leq |2\epsilon + c - c| + |2\epsilon + c - c| \\
    &= |2\epsilon| + |2\epsilon| \\
    &\leq 4\epsilon.
  \end{align*}

  Therefore $(a_n)$ is a Cauchy sequence in $\mathbb{R}$,
  and since $\mathbb{R}$ is complete, then $(a_n)$ is convergent.

\end{proof}

\newpage

\section*{1.5.2}
\begin{proof}
  $ $
  
  \setcounter{equation}{0}
  \begin{align}
    &\text{Assume } (x_i) \text{ is a Cauchy sequence in } X \nonumber \\
    \implies& \forall \epsilon > 0, \: \exists N \ni \forall j,k \geq N: \: 
    d(x_j, x_k) = \max_{i=1}^{n} |x^{(j)}_i - x^{(k)}_i| < \epsilon \\
    \implies& \forall \epsilon > 0, \: \exists N \ni \forall j,k \geq N, \: \forall i \in \{1,\ldots, n\}: 
    \:
    |x^{(j)}_i - x^{(k)}_i| < \epsilon \nonumber \nonumber \\
    \implies& \forall i \in \{1,\ldots, n\}: \: (x^{(1)}_i, x^{(2)}_i, \ldots) \text{ is a Cauchy sequence of real numbers} \nonumber \\
    \implies& \forall i \in \{1,\ldots, n\}: \: (x^{(1)}_i, x^{(2)}_i, \ldots) \text{ converges to a limit point } x_i \text{ because } \mathbb{R} \text{ is complete.} \nonumber
  \end{align}

  Next, we define a candidate limit for $(x_i)$:
  \begin{align*}
    x = (x_1, \ldots, x_n).
  \end{align*}

  Clearly, $x \in X$, and by (1) we have: 
  \begin{align*}
    \forall j \geq N: d(x_j, x) = \epsilon.
  \end{align*}

  This shows that $x$ is the limit of $(x_i)$ and proves completeness of $X$.

\end{proof}

\section*{1.5.5}
\begin{proof}
  $ $
  
  Assume $(x_n)$ is a Cauchy sequence in X

  \noindent
  $\implies \forall \epsilon > 0, 
  \: \exists N \ni \forall n,m \geq N:  
  \: d(x_n, x_m) = |x_n - x_m| < \epsilon$.
  \newline

  Take $\epsilon = 0.5$
  
  \noindent
  $\implies \exists N \ni \forall n,m \geq N: |x_n - x_m| < 0.5$

  \noindent
  $\implies \forall n,m \geq N: x_n = x_m$

  \noindent
  $\implies (x_n) \longrightarrow x_N$.

\end{proof}

\section*{1.5.6}
\begin{proof}(By Counterexample)
  $ $

  Take the sequence $x_n = n$.
  \newline

  First we show that it is Cauchy.
  \newline

  Given any $\epsilon > 0$, we can take $N(\epsilon) = \tan(\frac{\pi}{2} - \epsilon)$ so that $\forall n,m \geq N$:
  \begin{align*}
    &\arctan(x_m), \arctan(x_n) \in [\frac{\pi}{2} - \epsilon, {\frac{\pi}{2}}) \\
  \end{align*}
  \begin{align*}
    \implies d(x_n, x_m) &= |\arctan(x_n) - \arctan(x_m)| \\
    &\leq (\frac{\pi}{2} - \epsilon) - \frac{\pi}{2} \\
    &= \epsilon. \\
  \end{align*}

  Therefore $(x_n)$ is Cauchy as desired.
  \newline

  Now we show that $(x_n)$ does not converge. 
  
  We observe that the sequence wants to converge to $\frac{\pi}{2}$, since:
  \begin{align*}
    \lim\limits_{n \to \infty} d(x_n, \frac{\pi}{2}) = 0
  \end{align*}

  But there is no element $x \in \mathbb{R}$, such that $\arctan(x) = \frac{\pi}{2}$.
  \newline

  Thus our metric space is \textbf{\underline{incomplete}.}

\end{proof}

\section*{1.5.8}
\begin{proof}
  $ $

  \setcounter{equation}{0}
  \begin{align}
    &\text{Let } (x_n) \text{ be a Cauchy sequence in } Y \subseteq [a,b] \text{, and let J = [a,b]} \nonumber \\
    \implies& 
    \forall \epsilon>0, 
    \exists N 
    \ni \forall n,m \geq N: 
    d(x_n, x_m) = \max\limits_{t \in J} |x_n(t) - x_m(t)| < \epsilon \\
    \implies& 
    \forall \epsilon>0, 
    \exists N 
    \ni \forall n,m \geq N, 
    \: \forall t \in J: 
    |x_n(t) - x_m(t)| < \epsilon \nonumber \\
    \implies& 
    \forall t_0 \in J:
    \: (x_1(t_0), x_2(t_0), \ldots) \text{ is a Cauchy sequence of real numbers} \nonumber \\
    \implies& 
    \forall t_0 \in J:
    \: (x_1(t_0), x_2(t_0), \ldots) \text{ converges to a limit point, say } x_{lim}(t_0) \text{, because } \mathbb{R} \text{ is complete} \nonumber
  \end{align}

  Now, we define a candidate limit $x$ for $(x_n)$.
  \newline

  Define $x$ pointwise so that $\forall t_0 \in J: \: x(t_0) = x_{lim}(t_0)$.
  \newline

  From the defintion above, we obviously have: $x(a) = x(b)$.
  \newline

  From $(1)$ with $m \to \infty$ we have $\forall n > N$:
  \begin{align*}
    &d(x_n, x) = \max\limits_{t \in J} |x_n(t) - x(t)| < \epsilon \\
    \implies& \forall t_0 \in J: |x_n(t_0) - x(t_0) < \epsilon
  \end{align*}

  This shows that $x_m$ converges to $x$ \textbf{\underline{uniformly}} on $J$.
  \newline

  Since the $x_m$'s are continuous on $J$ and the convergence is uniform, the limit function $x$ is also continuous on $J$.
  \newline
  
  Because $x \in C[a,b]$ and $x(a) = x(b)$, then $x \in Y$, and thus $Y$ is complete.

\end{proof}

\newpage

\section*{2.1.5}
\begin{proof}
  $ $

  \begin{align*}
    &\sum_{i = 1}^{n} \alpha_i x_i 
    = 0_v \\
    \implies& \sum_{i = 1}^{n} \alpha_i t^i 
    = 0_v \\
    \implies& \forall i, \: \alpha_i = 0 \\
    \implies& (x_1, \ldots, x_n) \text{ is linearly independent.}
  \end{align*}

\end{proof}

\section*{2.1.6}
\begin{proof}(By Contradiction)
  $ $

  Let $x \in X$.

  Suppose $x$ has 2 different representations, $x_1$ and $x_2$, in the basis $(e_1,\ldots,e_n)$, where:
  \begin{align*}
    x_1 = \sum_{j = 1}^{n} \alpha_j e_j \qquad \text{and} \qquad
    x_2 = \sum_{k = 1}^{n} \beta_k e_k
  \end{align*}

  Then we have:
  \begin{align*}
    &x_1 = x_2 \\
    \implies& \sum_{j = 1}^{n} \alpha_j e_j = \sum_{k = 1}^{n} \beta e_k \\
    \implies& \forall i, \: \alpha_i = \beta_i
    \qquad \text{(Because } (e_1,\ldots,e_n) \text{ is linearly independent.)}
  \end{align*}

  This is a contradiction to our assumption that $x_1$ and $x_2$ have different representations. 
  
  Thus, we conclude that every non-zero vector must have a unique representation in a given basis.

\end{proof}


\section*{2.1.10}
\begin{proof}
  $ $

  \underline{i. $V = Y \cap Z$ is a subspace:}
  \newline

  We must check 3 conditions:

  \qquad 1. $0_v \in V$
  \begin{align*}
    & Y \text{ and } Z \text{ are vector spaces} \\
    \implies& 0_v \in Y \land 0_v \in Z \\
    \implies& 0_v \in V=Y \cap Z.
  \end{align*}

  \qquad 2. $v_1, v_2 \in V \implies v_1 + v_2 \in V$
  \begin{align*}
    &v1, v_2 \in V \\
    \implies& v_1, v_2 \in Y \land v_1, v_2 \in Z \\
    \implies& v_1 + v_2 \in Y \land v_1 + v_2 \in Z \\
    \implies& v_1 + v_2 \in V = Y \cup Z.
  \end{align*}

  \qquad 3. $k \in K$ and $v \in V \implies kv \in V$
  \begin{align*}
    &v \in V \\
    \implies& v \in Y \land v \in Z \\
    \implies& kv \in Y \land kv \in Z \\
    \implies& kv \in V = Y \cup Z.
  \end{align*}

  \underline{ii. $V = Y \cup Z$ is not a subspace:}
  \newline

  Consider the following counterexample:

  \quad Let $V = R^2$, $Y = \{(x, 0): x \in R\}$, $Z = \{ (0, y): y\in R \}$, and $V = Y \cup Z$.

  \quad Clearly, $Y$ and $Z$ are subspaces of $R^2$.

  \quad Take $v_1 = (1, 0)$ and $v_2 = (0, 1)$.

  \quad We have $v_1, v_2 \in V$, but $v_1 + v_2 = (1, 1) \not \in V$.

  \quad This shows that the union of subspaces fails to be closed under vector addition.
  
\end{proof}

\section*{2.1.11}
\begin{proof}
  $ $

  We must check 3 conditions:

  \qquad 1. $0_v \in M$
  \begin{align*}
    &\text{Let } v \in M \\
    \implies& 0v = 0_v \in M.
  \end{align*}

  \qquad 2. $v_1, v_2 \in M \implies v_1 + v_2 \in M$
  \begin{align*}
    &v1, v_2 \in M \\
    \implies& 
    v_1 = \sum_{\forall m_i \in M} \alpha_i m_i 
    \quad \text{ and } \quad
    v_2 = \sum_{\forall m_j \in M} \beta_j m_j \\
    \implies& v_1 + v_2 = \sum_{\forall m_k \in M} (\alpha_k + \beta_k) m_k 
    =
    \sum_{\forall m_k \in M} \gamma_k m_k \\
    \implies& v_1 + v_2 \in M.
  \end{align*}

  \qquad 3. $k \in K$ and $v \in M \implies kv \in M$
  \begin{align*}
    &v \in V \\
    \implies& v = \sum_{\forall m_i \in M} \alpha_i m_i  \\
    \implies& kv = \sum_{\forall m_i \in M} (k\alpha_i) m_i  \\
    \implies& kv = \sum_{\forall m_i \in M} \gamma_i m_i  \\
    \implies& kv \in M.
  \end{align*}

\end{proof}

\newpage

\section*{2.2.6}
\begin{proof}
  $ $

  Let $x = (\zeta_1, \zeta_2)$, and $y = (\eta_1, \eta_2)$.
  \newline

  \underline{i. $||x||_1 = |\zeta_1| + |\zeta_2|$}
  \newline

  N1 holds because:
  \begin{align*}
    ||x||_1 = |\zeta_1| + |\zeta_2| \geq 0.
  \end{align*}

  N2 holds because:

  \underline{$\implies$:}
  \begin{align*}
    & ||x||_1 = 0 \\
    \implies& |\zeta_1| + |\zeta_2| = 0 \\
    \implies& |\zeta_1|, |\zeta_2| = 0 \\
    \implies& \zeta_1, \zeta_2 = 0 \\
    \implies& x = (0, 0) = 0_v.
  \end{align*}

  \underline{$\impliedby$:}
  \begin{align*}
    & x = 0 \\
    \implies& \zeta_1, \zeta_2 = 0 \\
    \implies& |\zeta_1|, |\zeta_2| = 0 \\
    \implies& |\zeta_1| + |\zeta_2| = 0 \\
    \implies& ||x||_1 = 0 \\
  \end{align*}

  N3 holds because:
  \begin{align*}
    ||\alpha x||_1 
    &= |\alpha \zeta_1| + |\alpha \zeta_2| \\
    &= |\alpha| |\zeta_1| + |\alpha| |\zeta_2| \\
    &= |\alpha| (|\zeta_1| + |\zeta_2|) \\
    &= |\alpha| \: ||x||_1 \\
  \end{align*}

  N4 holds because:
  \begin{align*}
    ||x+y||_1
    &= |\zeta_1 + \eta_1| + |\zeta_2 + \eta_2| \\
    &\leq |\zeta_1| + |\eta_1| + |\zeta_2| + |\eta_2| \\
    &= (|\zeta_1| + |\zeta_2|) + (|\eta_1| + |\eta_2|) \\
    &= ||x||_1 + ||y||_1.
  \end{align*}

  \underline{ii. $||x||_2 = (\zeta_1^2 + \zeta_2^2)^{\frac{1}{2}}$}
  \newline
  
  N1 holds because:
  \begin{align*}
    ||x||_2 = (|\zeta_1|^2 + |\zeta_2|^2)^{\frac{1}{2}} \geq 0.
  \end{align*}

  N2 holds because:

  \underline{$\implies$:}
  \begin{align*}
    & ||x||_2 = 0 \\
    \implies& (|\zeta_1|^2 + |\zeta_2|^2)^\frac{1}{2} = 0 \\
    \implies& |\zeta_1|^2 + |\zeta_2|^2 = 0 \\
    \implies& |\zeta_1|^2, |\zeta_2|^2 = 0 \\
    \implies& |\zeta_1|, |\zeta_2| = 0 \\
    \implies& \zeta_1, \zeta_2 = 0 \\
    \implies& x = (0, 0) = 0_v.
  \end{align*}

  \underline{$\impliedby$:}
  \begin{align*}
    & x = 0 \\
    \implies& \zeta_1, \zeta_2 = 0 \\
    \implies& |\zeta_1|, |\zeta_2| = 0 \\
    \implies& |\zeta_1|^2, |\zeta_2|^2 = 0 \\
    \implies& |\zeta_1|^2 + |\zeta_2|^2 = 0 \\
    \implies& (|\zeta_1|^2 + |\zeta_2|^2)^\frac{1}{2} = 0 \\
    \implies& ||x||_2 = 0 \\
  \end{align*}

  N3 holds because:
  \begin{align*}
    ||\alpha x||_2
    &= (|\alpha \zeta_1|^2 + |\alpha \zeta_2|^2)^\frac{1}{2} \\
    &= (|\alpha|^2 |\zeta_1|^2 + |\alpha|^2 |\zeta_2|^2)^\frac{1}{2} \\
    &= |\alpha| (|\zeta_1|^2 + |\zeta_2|^2)^\frac{1}{2} \\
    &= |\alpha| \: ||x||_2 \\
  \end{align*}

  N4 holds because:
  \begin{align*}
    ||x+y||_2^2
    &= (\zeta_1 + \eta_1)^2 + (\zeta_2 + \eta_2)^2 \\
    &= \zeta_1^2 + 2\zeta_1 \eta_1 + \eta_1^2
    + \zeta_2^2 + 2\zeta_2 \eta_2 + \eta_2^2 \\
    &= (\zeta_1^2 + \zeta_2^2) 
    + (\eta_1^2 + \eta_2^2)
    + 2(\zeta_1 \eta_1 + \zeta_2 \eta_2) \\
    &\leq (\zeta_1^2 + \zeta_2^2) 
    + (\eta_1^2 + \eta_2^2)
    + 2\sqrt{\zeta_1^2 + \zeta_2^2} \sqrt{\eta_1^2 + \eta_2^2} \qquad \text{(By the Cauchy-Schwartz inequality)} \\
    &= ||x||_2^2 + ||y||_2^2 +  2||x||_2 \: ||y||_2 \\
    &= (||x||_2 + ||y||_2)^2
  \end{align*}

  \noindent
  $\implies ||x+y||_2 \leq ||x||_2 + ||y||_2$.
  \newline

  \underline{iii. $||x||_\infty = \max \lbrace |\zeta_1|, |\zeta_2| \rbrace$}
  \newline
  
  N1 holds because:
  \begin{align*}
    ||x||_\infty = \max \lbrace |\zeta_1|, |\zeta_2| \rbrace \geq 0.
  \end{align*}

  N2 holds because:

  \underline{$\implies$:}
  \begin{align*}
    & ||x||_\infty = 0 \\
    \implies& \max \lbrace |\zeta_1|, |\zeta_2| \rbrace = 0 \\
    \implies& |\zeta_1|, |\zeta_2| = 0 \\
    \implies& \zeta_1, \zeta_2 = 0 \\
    \implies& x = (\zeta_1, \zeta_2) = (0, 0) = 0_v.
  \end{align*}

  \underline{$\impliedby$:}
  \begin{align*}
    & x = 0 \\
    \implies& \zeta_1, \zeta_2 = 0 \\
    \implies& |\zeta_1|, |\zeta_2| = 0 \\
    \implies& \max \lbrace |\zeta_1|, |\zeta_2| \rbrace = 0 \\
    \implies& ||x||_\infty = 0 \\
  \end{align*}

  N3 holds because:
  \begin{align*}
    ||\alpha x||_\infty
    &= \max \lbrace |\alpha \zeta_1|, 
    |\alpha \zeta_2| \rbrace \\
    &= |\alpha| \: \max \lbrace |\zeta_1|, 
    |\zeta_2| \rbrace \\
    &= |\alpha| \: ||x||_\infty \\
  \end{align*}

  N4 holds because:
  \begin{align*}
    ||x+y||_\infty
    &= \max \lbrace |\zeta_1 + \eta_2|, 
    |\zeta_2 + \eta_2| \rbrace \\
    &\leq \max \lbrace |\zeta_1| + |\eta_2|, 
    |\zeta_2| + |\eta_2| \rbrace \\
    &\leq \max \lbrace |\zeta_1|, 
    |\zeta_2| \rbrace 
    + \max \lbrace |\eta_1|, 
    |\eta_2| \rbrace \\
    &= ||x||_\infty + ||y||_\infty.
  \end{align*}
  
\end{proof}

\section*{2.2.8}

\newpage

\section*{2.6.2}
\begin{proof}
  $ $

  Let $v_1, v_2 \in R^2$, where $v_1 = (\zeta_1, \zeta_2)$, $v_2 = (\eta_1, \eta_2)$, and $\alpha \in R$.
  \newline

  \underline{i. $T_1: (\zeta_1, \zeta_2) \longmapsto (\zeta_1, 0)$}
  \newline

  Additivity:
  \begin{align*}
    T_1 (v_1 + v_2) 
    &= T_1 (\zeta_1 + \eta_1, \zeta_2 + \eta_2) \\
    &= (\zeta_1 + \eta_1, 0) \\
    &= (\zeta_1, 0) + (\eta_1, 0) \\
    &= T_1 v_1 + T_1 v_2.
  \end{align*}

  Homogeneity:
  \begin{align*}
    T_1 (\alpha v_1) 
    &= T_1 (\alpha \zeta_1, \alpha \zeta_2) \\
    &= (\alpha \zeta_1, 0) \\
    &= \alpha (\zeta_1, 0) \\
    &= \alpha T_1 v_1.
  \end{align*}

  Geometric interpretation: Projection onto the x-axis.
  \newline

  \underline{ii. $T_2: (\zeta_1, \zeta_2) \longmapsto (0, \zeta_2)$}
  \newline

  Proof is similar to part (i).
  \newline

  Geometric interpretation: Projection onto the y-axis.
  \newline

  \underline{iii. $T_3: (\zeta_1, \zeta_2) \longmapsto (\zeta_2, \zeta_1)$}
  \newline

  Additivity:
  \begin{align*}
    T_3(v_1 + v_2) 
    &= T_3 (\zeta_1 + \eta_1, \zeta_2 + \eta_2) \\
    &= (\zeta_2 + \eta_2, \zeta_1 + \eta_1) \\
    &= (\zeta_2, \zeta_1)
    +
    (\eta_2, \eta_1) \\
    &= T_3 v_1 + T_3 v_2.
  \end{align*}

  Homogeneity:
  \begin{align*}
    T_3 (\alpha v_1)
    &= T_3 (\alpha \zeta_1, \alpha \zeta_2) \\
    &= (\alpha \zeta_2, \alpha \zeta_1) \\
    &= \alpha (\zeta_2, \zeta_1) \\
    &= \alpha T_3 v_1
  \end{align*}

  Geometric interpretation: Reflection across the line $y=x$.
  \newline

  \underline{iv. $T_4: (\zeta_1, \zeta_2) \longmapsto (\gamma \zeta_1, \gamma \zeta_2)$}
  \newline

  Additivity:
  \begin{align*}
    T_4 (v_1 + v_2)
    &= T_4 (\zeta_1 + \eta_1, \zeta_2 + \eta_2) \\
    &= (\gamma (\zeta_1 + \eta_1), \gamma (\zeta_2 + \eta_2)) \\
    &= (\gamma \zeta_1, \gamma \zeta_2) 
    +
    (\gamma \eta_1, \gamma \eta_2)\\
    &= T_4 v_1 + T_4 v_2.
  \end{align*}

  Homogeneity:
  \begin{align*}
    T_4 (\alpha v_1)
    &= T_4 (\alpha \zeta_1, \alpha \zeta_2) \\
    &= (\gamma \alpha \zeta_1, \gamma \alpha \zeta_2) \\
    &= \alpha (\gamma \zeta_1, \gamma \zeta_2) \\
    &= \alpha T_4 v_1
  \end{align*}

\end{proof}

\section*{2.6.6}
\begin{proof}
  $ $
  
  Let $X, Y, Z$ be vector spaces over the same field $K$.

  Let $T: X \longrightarrow Y$, $S: Y \longrightarrow Z$.
  
  Then, the composite operator $ST$ exists and $ST: X \longrightarrow Z$.
  \newline

  Let $x_1, x_2 \in X$, and $\alpha \in K$.
  \newline

  Additivity:
  \begin{align*}
    (ST) (x_1 + x_2)
    &= S (T (x_1 + x_2)) \\
    &= S (T x_1 + T x_2) \\
    &= (ST) x_1 + (ST) x_2.
  \end{align*}

  Homogeneity:
  \begin{align*}
    (ST) (\alpha x_1)
    &= S (T (\alpha x_1)) \\
    &= S (\alpha  T x_1) \\
    &= \alpha (ST) x_1.
  \end{align*}

\end{proof}

\newpage

\section*{2.7.2}
\begin{proof}
  $ $

  \underline{$\implies: $}
  
  Let $T: X \longrightarrow Y$ be a bounded linear operator, and $B_x$ be a bounded set in $X$.
  \newline

  Since $B_x$ is bounded, then $\forall x \in B_x$:
  \begin{align*}
    ||x|| \leq c_x.
  \end{align*}

  Let $B_y$ be the image of $B_x$ under $T$: $B_y= T(B_x)$.
  \newline

  Since $T$ is bounded, then $\forall y \in B_y$:
  \begin{align*}
    ||y|| &= ||Tx|| \\
    &\leq ||T|| \: ||x|| \\
    &\leq ||T||  c_x = c_y.
  \end{align*}

  Thus, $B_y$ is bounded.
  \newline

  \underline{$\impliedby: $}

  Suppose $T$ maps bounded sets in $X$ into bounded sets $Y$.
  \newline

  Let $B_x = \{x \in X: ||x|| = 1 \}$.
  
  \noindent
  $\implies B_x$ is bounded.
  \newline

  Let $B_y$ be the image of $B_x$ under $T$: $B_y = T(B_x)$.
  
  \noindent
  $\implies B_y$ is bounded
  
  \noindent
  $\implies \forall y \in B_y: \: ||y|| \leq c$

  \noindent
  $\implies \forall x \in B_x: ||Tx|| \leq c$

  \noindent
  $\implies T $ is bounded by $Lemma$  $2.7-2$.

\end{proof}


\section*{2.7.3}
\begin{proof}
  $ $
  
  Let $x \in X$ s.t. $||x|| = \alpha < 1$, then:
  \begin{align*}
    &y = \frac{1}{\alpha} x \\
    \implies& ||y|| = \frac{1}{\alpha} ||x|| = 1 \\
    \implies& ||Ty|| \leq ||T|| \\
    \implies& ||T (\frac{1}{\alpha} x)|| \leq ||T|| \\
    \implies& \frac{1}{\alpha} ||Tx|| \leq ||T|| \\
    \implies& ||Tx|| \leq \alpha ||T|| < ||T||.
  \end{align*}
  
\end{proof}

\newpage


\section*{3.1.4}
\begin{proof}
  \begin{align*}
    \langle x+y, x-y \rangle 
    &= \langle x, x-y \rangle
    +
    \langle y, x-y \rangle \\
    &= \langle x, x \rangle - \langle x, y \rangle
    +
    \langle y, x \rangle - \langle y, y \rangle \\
    &= 0 - \langle x, y \rangle
    +
    \langle x, y \rangle - 0
    = 0 \\
    &= 0.
  \end{align*}

  Geometric interpretation for $X = \mathbb{R}^2$:
  
  \qquad Since $||x|| = ||y||$, then the vectors $x$ and $y$ form a rhombus. The vectors $x+y$ and $x-y$ are the diagonals of that rhombus.
  
  \qquad Therefore, this statement simply states that the diagonals of a rhombus are perpendicular/orthogonal.
  \newline

  Geometric interpretation for $X = \mathbb{C}^1$: 

  \qquad Same as $X = \mathbb{R}^2$, since $\mathbb{R}^2$ and $\mathbb{C}^1$ have the same geometry.




\end{proof}

\section*{3.1.7}
\begin{proof}(By Contradiction)
  $ $

  Let $u, v \in V$ s.t. $u \neq 0_v$ and $v := a u$, where $a \neq 1$. Then $u \neq v$, and:
  \begin{align*}
    &\forall x \in X: \langle x, u \rangle = \langle x, v \rangle \\
    \implies& \forall x \in X: \langle x, u \rangle = \langle x, au \rangle \\
    \implies& \forall x \in X: \langle x, u \rangle = \bar{a} \langle x, u \rangle
  \end{align*}

  But this is impossible since $a \neq 1$. Thus, we must have $u = v$.

\end{proof}

\section*{3.1.11}
\begin{proof}
  $ $

  To check if the given norm is induced by some inner-product, we could check whether the parallelogram equality holds, but there's an even simpler proof.
  \newline

  First, we note that this space is an $l^p$ space with $p=1$.
  \newline

  Finally, we note that it was shown in $3.1-7$ that the space $l^p$ with $p \neq 2$ is not an inner-product space.
  \newline
  
  Therefore, there is no inner-product that induces the given norm.

\end{proof}

\newpage


\section*{3.2.1}
$ $

The Euclidean inner product on $\mathbb{R}^2$ and $\mathbb{R}^3$ reduces to the dot product.
\newline

The dot product of 2 vectors is the product of their length times cosine the angle between them:
\begin{align*}
  \langle x,y \rangle
  &= x \cdot y \\
  &= ||x|| \: ||y|| \cos(\theta)
\end{align*}

Taking the absolute value of both sides we get:
\begin{align*}
  |\langle x,y \rangle| 
  &= ||x|| \: ||y|| \: |\cos(\theta)| \\
  &\leq ||x|| \: ||y||
\end{align*}


Which is the same result given by the Schwarz inequality.


\section*{3.2.2}
$ $

i. The trivial subspace containing just the zero sequence.

ii. Subspaces whose components are all $0$ after some index $n$.


\section*{3.2.4}
\setcounter{equation}{0}
\begin{proof}
  $ $

  By the continuity of the inner product (Lemma 3.2-2), we have:
  \begin{align}
    \langle x_n, y \rangle \to \langle x, y \rangle \label{3.2.4.1}
  \end{align}
  
  Next, we define the sequence $(a_n)$ by:  $a_n := \langle x_n, y \rangle$. Then:
  \begin{align}
    & \forall n \in \mathbb{N}: \: a_n = 0 \nonumber \\
    \implies& a_n \to 0 \nonumber \\
    \implies& \langle x_n, y \rangle \to 0 \label{3.2.4.2}
  \end{align}

  Finally:
  \begin{align*}
    (\ref{3.2.4.1}) \text{ and } (\ref{3.2.4.2}) \implies& \langle x, y \rangle \to 0 \\
    \implies& x \perp y.
  \end{align*}

\end{proof}

\section*{3.2.5}
\begin{proof}
  $ $

  To prove that $x_n \to x$, we will show that $||x_n - x|| \to 0$:
  \begin{align*}
    ||x_n - x|| 
    &= \langle x_n - x, x_n -x \rangle \\
    &= \langle x_n, x_n \rangle
    - \langle x_n, x \rangle
    - \langle x, x_n \rangle
    + \langle x, x \rangle \\
    &= ||x_n||^2
    - \langle x_n, x \rangle
    - \langle x, x_n \rangle
    + ||x_n||^2
  \end{align*}

  Taking the limit as $n \to \infty$:
  \begin{align*}
    ||x_n - x|| 
    \to
    2 ||x||^2
    - 2\langle x, x \rangle = 2 ||x||^2 - 2 ||x||^2 = 0.
  \end{align*}
\end{proof}

\section*{3.2.8}
\begin{proof}
  $ $

  \underline{$\implies$}:
  \begin{align*}
    & \langle x, y \rangle = 0 \\
    \implies& \forall \bar{\alpha}: \: \bar{\alpha} \langle x, y \rangle = 0 \\
    \implies& \forall \alpha: \: \langle x, \alpha y \rangle = 0 \\
    \implies& \forall \alpha: \: \langle x, \alpha y \rangle = 0 \: \land \: \langle \alpha y, x \rangle = 0
  \end{align*}

  Next, we have $\forall \alpha$:
  \begin{align*}
    ||x + \alpha y||^2 
    &= \langle x + \alpha y, x + \alpha y \rangle \\
    &= 
    \langle x, x \rangle
    +
    \langle x, \alpha y \rangle
    +
    \langle \alpha y, x \rangle
    +
    \langle y, y \rangle \\
    &= 
    ||x||^2
    +
    0
    +
    0
    +
    ||y||^2 \\
    &\geq ||x||^2
  \end{align*}

  Finally, taking the square root, we arrive at our desired result:
  \begin{align*}
    ||x + \alpha y|| \geq ||x||.
  \end{align*}

  \underline{$\impliedby$}:
  \begin{alignat*}{2}
    & \forall \alpha: \: ||x|| &&\leq ||x + \alpha y||  \\
    \implies& \forall \alpha: \: ||x||^2 &&\leq ||x + \alpha y||^2 \\
    & &&= \langle x + \alpha y, x + \alpha y \rangle \\
    & &&= \langle x, x \rangle
    +
    \langle x, \alpha y \rangle
    +
    \langle \alpha y, x \rangle
    +
    \langle y, y \rangle \\
    & &&= 
    ||x||^2
    +
   \overline{\langle \alpha y, x \rangle}
    +
    \langle \alpha y, x \rangle
    +
    ||y||^2 \\
    & &&= 
    ||x||^2
    +
    2Re\langle \alpha y, x \rangle
    +
    ||y||^2 \\
    & &&= 
    ||x||^2
    +
    2Re (\alpha \langle y, x \rangle)
    +
    ||y||^2 \\
    \implies& \forall \alpha: \: 2Re\langle \alpha y, x \rangle &&\leq 0 \\
    \implies& \langle x, y \rangle = 0.
  \end{alignat*}

\end{proof}

\newpage

\section*{5.1.2}
\begin{proof}
  $ $

  Let $x,y \in X$
  \begin{align*}
    d(Tx, Ty) 
    &= |Tx - Ty| \\
    &= |\frac{x}{2} + x^{-1} - \frac{y}{2} - y^{-1}| \\
    &= |\frac{x-y}{2} + \frac{y-x}{xy}| \\
    &= |x-y| \: |\frac{1}{2} - \frac{1}{xy}| \\
    &\leq \frac{1}{2} \: |x-y| \\
    &= \frac{1}{2} d(x,y)
  \end{align*}

  Therefore, $T$ is a contraction and the smallest $\alpha$ is $\frac{1}{2}$.

\end{proof}

\section*{5.1.5}
\setcounter{equation}{0}
\begin{proof} (By Contradiction)
  $ $

  Suppose $T$ has 2 different fixed points $x$ and $y$, then:
  \begin{align}
    & x = Tx \land y = Ty \nonumber \\
    \implies& d(x, y) = d(Tx, Ty) 
  \end{align}

  But we are given that:
  \begin{align}
    d(Tx, Ty) < d(x, y)
  \end{align}

  (1) and (2) $\implies d(x,y) < d(x,y)$.
  \newline

  This is a contradiction. Thus, $T$ has a unique fixed point.

\end{proof}


\section*{5.1.6}
\setcounter{equation}{0}

\underline{$i. \: T \text{ is a contraction} \implies T^n \text{ is a contraction}$:}
\begin{proof}(By Induction)

  \underline{Base case $(n = 1)$:}

  $T^1 = T$ which is a contraction by assumption.
  \newline

  \underline{Inductive step ($n = k+1$):}

  Suppose the induction hypothesis holds for $n = k$, and let $x, y \in X$, then:
  \begin{align}
    d(T^k x, T^k y) \leq \beta \: d(x, y) \qquad \text{(where $0 < \beta < 1$)}
  \end{align}

  Next, let $T^kx = u$ and $T^k y = v$. Because $T$ is a contraction then:
  \begin{align}
    d(Tu, Tv) \leq \alpha \: d(u, v) \qquad \text{(where $0 < \alpha < 1$)}
  \end{align} 

  Putting $(1)$ and $(2)$ together, we get: 
  \begin{align*}
    d(T^{k+1}x, T^{k+1}y) &= d(Tu, Tv) \\
    &\leq \alpha \: d(u, v) \\
    &\leq \alpha \: \beta \: d(x,y) \\
    &= \gamma \: d(x,y) \qquad \text{(where $0 < \gamma < 1$)}
  \end{align*}

  Therefore, $T^{k+1}$ is a contraction.

\end{proof}

\noindent
\underline{$ii. \: T^n \text{ is a contraction} \centernot\implies T \text{ is a contraction}$:}
\begin{proof}(By Counterexample)
  $ $
  
  Let the mapping $T^n: R \longrightarrow R$ be defined by:
  \begin{align*}
    Tx := \begin{cases}
      -x, &\text{if } x \geq 0 \\
      \frac{x}{2}, &\text{if } x < 0
    \end{cases}
  \end{align*}

  Then, $T$ does not contract the distance between positive numbers.

  However, $T^2$ is a contraction.

\end{proof}

\end{document}